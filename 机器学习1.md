## 监督学习

数据集中每个数据都有正确答案。

回归问题：对连续的数据学习，预测的输出也是连续的。（线性回归算法）

分类问题：对离散的数据学习，预测的输出也是离散的。（逻辑回归算法）

## 无监督学习

数据集没有标签，程序自动分类出不同数据结构的聚集族（聚类算法）。

> 监督学习：垃圾邮件判断（分类问题）、疾病判断（分类问题）、房价预测（回归问题）。

> 无监督学习：新闻事件分类、市场分类。

## 单变量线性回归

只有一个特征x：h(x) = θ1 + θ2 * x

> 其中x代表特征（单变量），θ1和θ2是要训练的参数。

## 损失函数

J(a, b) = 1/2m  * Σ [(h(x) - y(x)) ^ 2]

> 对有m个数据的数据集，求他m个预测数据与实际数据的差值的累加和。<br>
> 损失函数J最小对应的θ1和θ2的参数为最佳参数。<br>

## 梯度下降

![梯度下降](img/梯度下降.jpg)

梯度下降的目标：找到一组使函数与实际数据拟合时误差最小的参数（即损失函数最小）

缺点：没有尝试所有参数，使用梯度下降找到的损失函数最小值为局部最小值。如上图所示。

对所有参数批量梯度下降的公式如下：

实际应用中θ可能有成千上万个（截至2021.1.14最大的模型时google的Switch Transformer语言模型，有16000亿参数）

![批量梯度下降](img/批量梯度下降.png)

公式中的α表示学习率（即下降速率），公式的含义：旧的参数 - （旧参数的斜率 * 学习率）

## 线性回归种的梯度下降

下图分别展示了批量梯度下降算法和线性回归算法。

![梯度下降和线性回归](img/梯度下降和线性回归.png)

两个算法融合后的算法如下（已经经过简单的求导处理）：

![线性回归的梯度下降算法](img/线性回归的梯度下降算法.png)

## 多变量线性回归

多变量线性回归和单变量线性回归的区别：

多变量：每个训练实例的特征是多维的。

单变量：每个训练实例的特征是一维的。

## 多变量线性回归梯度下降

与单变量线性回归梯度下降的区别：

单变量：每次下降更新θ0和θ1。

多变量：每次下降更行θ0~θn。

## 多变量线性回归梯度下降实例

1. 特征缩放

下图中，左图为特征缩放前，右图为特征缩放后。

特征缩放的目的：帮助梯度下降更快收敛。

![特征缩放](img/特征缩放.png)

2. 学习率

每次梯度下降都会受到学习率α的影响。

α过大：在损失函数最小值处震荡，无法收敛。

α过小：收敛速度过慢。

常用α：0.01，0.03，0.1，0.3，1，3，10

## 特征和多项式回归

线性回归并不适用与所有数据，有时需用曲线来适应数据，也就是多项式回归模型。

example: h(x) = θ0 + θ1 * x^2 + θ2 * x^3 + ····· + θn * x^(n+1)

采用多项式回归模型需要进行特征缩放，不然不同特征间的数量级可能差几百万倍
